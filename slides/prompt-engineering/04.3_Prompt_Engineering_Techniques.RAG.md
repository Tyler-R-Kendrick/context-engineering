---
title: "Prompt Engineering Techniques"
module: "01 - Prompt Engineering"
order: 4
tags:
  - prompts
  - sdk
duration: "30min"
marp: true
theme: default
paginate: true
doc-type: content

header: Introduction to Prompt Engineering
footer: "Â© Microsoft Corporation. All rights reserved."
style: |
  @import '../styles/msft.css';

  section {
    overflow-y: scroll;
  }

---

<!-- _class: 'title-slide' -->
<!-- _paginate: skip -->

# Prompt Engineering Techniques

## Retrieval-Augmented Generation (RAG)

---

### Naive RAG / Cache Augmented Generation (CAG)

This process simply includes all of the additional context that you need in the context window for the LLM. Instead of trying to intelligently include only the most relevant content, including the entire document allows for the LLM to utilize the full context and infer what is more relevant.

***Benefits***: This simplifies retrieval and solution design.
***Detriments***:
 * With sufficiently long contexts, current llms suffer from the "lost-in-the-middle" problem.
 * This only works if you can fit the entire document in the context window.

---

### Self-RAG

This technique was introduced to allow a single model to self-determine if its own knowledge could be trusted. By fine-tuning a model to classify the 3 tasks (retrieve, generate, and critique), along with outputting special "reflection tokens" that signal which tasks to engage in, an llm can signal when its training set is insufficient and reduce hallucinations.

***Benefits***: Allows an llm to act as its own fact-checking agent.
***Detriments***: Requires fine-tuning - which can be costly.

---

### Vector RAG

By using a vector embedding model, inputs can be converted into vectors that are then used to search a vector database for relevance.

***Benefits*** Allows for semantic similarity search.
***Detriments***
 * The entire indexed dataset needs to be re-indexed if you change the embedding model used.
 * Chunking strategies need to be designed to produce meaningful embeddings.
 * Indexed search results must be semantically similar to the query - not necessarily an expected answer (though this can be addressed through techniques like HyDe.)
 * "Local" queries about a document can be answered, but "global" queries about a dataset cannot be.

---

### Graph RAG

Created by Microsoft Research, graphRAG uses an llm to create and map knowledge to an ontology - and create community summaries using a clustering algorithm to generate aggregate summaries of node/edge relationships.

**Benefits**: Answers "global" questions about a dataset instead of exclusively "local" questions about documents.
**Detriments** Ingestion is much more costly and retrieval becomes more time consuming to traverse the graph.

---

### Path RAG

Advances on GraphRAG techniques to use node/edge descriptions as natual language answers to user queries.

**Benefits**: Achieves similar results to graphRAG, with far less compute time.
**Detriments**: Doesn't build community summaries, necessitating graph traversal to process "global" questions.

---

### Agentic RAG

By treating agents as a Mixture-of-Experts with domain expertise, an LLM can retrieve information relevant to the conversation context through interacting with other agents for the retrieval task, instead of their own toolsets.

**Benefits**: Makes maintaining/scaling agents much simpler.
**Detriments**: Becomes more difficult to debug/trace.

