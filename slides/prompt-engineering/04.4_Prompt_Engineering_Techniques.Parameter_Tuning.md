---
title: "Prompt Engineering Techniques"
module: "01 - Prompt Engineering"
order: 4
tags:
  - prompts
  - sdk
duration: "30min"
marp: true
theme: default
paginate: true
doc-type: content

header: Introduction to Prompt Engineering
footer: "© Microsoft Corporation. All rights reserved."
style: |
  @import '../styles/msft.css';

  section {
    overflow-y: scroll;
  }

---

<!-- _class: 'title-slide' -->
<!-- _paginate: skip -->

# Prompt Engineering Techniques

## Advanced Techniques

---


### Prompt Chaining

**Definition:** Connecting several prompts so the output of one becomes the input to the next. This enables complex, multi-step workflows in which each prompt handles a distinct sub-task.

**Concrete Uses:**
- Decomposing a challenging question into simpler parts, with separate prompts for each step.
- Popular with tasks like legal document analysis, multi-part data extraction, and multi-stage conversation agents.

**Advantages:** Increases modularity, allows for iterative refinement, and makes error diagnosis easier by identifying failing steps within a chain.

---

### Self-consistency and Reflexion

---

**Self-consistency:** Improves reliability by running a prompt multiple times (using a high temperature for diversity) and then aggregating answers; the most frequent (self-consistent) result is chosen.

**Steps:**
1. Generate multiple outputs using the same prompt (encourage reasoning diversity).
2. Extract answers from each.
3. Vote/aggregate to choose the most common answer.

**Example:** Classify an ambiguous email repeatedly; if the majority of runs say "IMPORTANT", that label is returned.

---

**Reflexion:** Prompt the model to critique or revise its own output, iteratively. Often involves metaprompts that ask for error checking, self-assessment, or explicit correction strategies.

---

## Grounding, Structured Output, & Constrained Decoding

---

**Grounding:**
Grounding refers to ensuring that model outputs are based on verifiable facts or external data, rather than just the model's internal knowledge. This is crucial for applications requiring high factual accuracy.

---

**Structured Output:**
Design prompts to elicit responses in predefined, machine-readable formats (such as JSON, XML, tables). Vital for integration with downstream applications or automated data pipelines.

---

**Constrained Decoding:**
Imposes syntactic, semantic, or business-rules constraints on output (e.g., requiring a valid JSON schema, restricting vocabulary, etc.) Useful for compliance, safety, and ensuring model responses can be programmatically parsed and trusted. Effective prompts balance clear instructions (what to do) and explicit constraints (what not to do).

---

## External Tools Integration
**Tool Use (ReAct, LangChain, Plugins):**
Prompts direct LLMs to invoke tools, APIs, web search, code interpreters, databases, etc. Approaches like ReAct combine language reasoning with actions—LLM thinks, acts (calls a tool/API), observes results, and iterates.

**Example:** Have the LLM search Google or execute code snippets as part of an answer; each reasoning step can update based on retrieved evidence.

**Orchestration and Workflows:**
Orchestration refers to coordinating multiple tools, prompts, or models in a workflow to solve complex tasks, often using frameworks or pipelines for automation and reliability.

---

## Parameter Tuning

Careful parameter tuning is essential for effective prompt engineering with large language models (LLMs). Adjusting these parameters as part of the LLM output configuration process lets you control the randomness, determinism, creativity, and reproducibility of your model outputs for your specific use case.

---

### Temperature and Top_p

---

**Temperature:**

Temperature controls the degree of randomness in the model's token selection.

- **Low temperature (e.g., 0.0 - 0.2):** The model outputs are deterministic and focused; the highest probability token is always selected. Suitable for tasks requiring accuracy and consistency (e.g., fact extraction, math problems).
- **Medium temperature (e.g., 0.5):** Allows for some variation, balancing predictability and creativity.
- **High temperature (e.g., 0.9 or above):** Outputs become more diverse and creative with increased randomness, but risk producing less relevant or off-topic text.

**Best practice:** For tasks requiring precise answers, set temperature to 0. For creative tasks, start with 0.7–0.9 and adjust based on observed output.

---

**Top_p (Nucleus Sampling):**

Top_p restricts possible next tokens to a subset accumulating to the top p probability mass.

- **Low top_p (e.g., 0.8 - 0.9):** Results in more focused, deterministic outputs.
- **High top_p (close to 1):** Makes every likely token eligible, increasing diversity.

**Interaction with temperature and top_k:** These controls can be used together, but at extremes (e.g., top_p = 1 or temperature = 0), the other settings become irrelevant.

---

#### Practical Starting Points

- For coherent and moderately creative results: `temperature = 0.2`, `top_p = 0.95`
- For maximum creativity: `temperature = 0.9`, `top_p = 0.99`
- For deterministic, fact-based tasks: `temperature = 0`, `top_p = 1`

> **Tip:** Too much randomness (high temperature and/or top_p) can cause the model to generate loops or filler text, while too little can make outputs monotonous or repetitive. Always experiment and iterate for your task.

---

### Seed Values

**Purpose:**

Seed values ensure the reproducibility of model outputs. When you set the same seed, you will (with some exceptions, e.g., model updates) get the same output for a given prompt and model configuration.

This is especially useful for testing, debugging, or creating teaching materials, as it removes randomness from the workflow.

> **Note:** Not all platforms expose a seed parameter to users. Where available, setting a seed is highly recommended for reproducibility in demonstrations or evaluations.

---

### Model-specific Parameters

---

**Parameter Diversity:**

Each LLM and platform may offer different configuration settings beyond just temperature and top_p/top_k. Common additional parameters include:

- **Max tokens/output length:** Controls how many tokens the LLM generates. More tokens allow for longer responses but increase computational cost and potential drift in responses.
- **Top_k:** Chooses from only the k most likely candidates for the next token. A smaller k makes output more deterministic; a larger k increases possible variety.
- **Stop sequences:** Specify strings at which the output should end, helpful for structured outputs.
- **Specialized parameters:** Some models/platforms offer extra configuration, e.g., controlling system or role behavior, response formatting, or safety/toxicity filters.

---

**Model Behavior:**

The impact of parameter values may differ between models. Always:

- Review documentation for your chosen LLM/platform.
- Start with recommended/default settings and iteratively adjust.
- Document settings, results, and iterations to track what works best for your use case.

---

### Best Practice

Craft and test different prompts, analyze and document the results. Refine prompt design and configuration based on performance. When changing parameters or model, revisit previously used prompts and keep experimenting until the desired output is achieved.
