{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Engineering with GitHub Models\n",
    "\n",
    "This notebook demonstrates prompt engineering concepts using GitHub's Azure AI Inference service.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Set `GITHUB_TOKEN` environment variable with a GitHub Personal Access Token\n",
    "- Install required packages: `pip install azure-ai-inference`\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import SystemMessage, UserMessage\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "# Get GitHub token from environment\n",
    "github_token = os.environ.get(\"GITHUB_TOKEN\")\n",
    "if not github_token:\n",
    "    raise ValueError(\"GITHUB_TOKEN environment variable must be set\")\n",
    "\n",
    "# Initialize client\n",
    "endpoint = \"https://models.github.ai/inference\"\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(github_token)\n",
    ")\n",
    "\n",
    "# Choose a model (GPT-4o, GPT-4o-mini, etc.)\n",
    "model = \"gpt-4o-mini\"\n",
    "\n",
    "print(f\"âœ… Connected to {endpoint}\")\n",
    "print(f\"ðŸ“¦ Using model: {model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Vague vs. Specific Prompts\n",
    "\n",
    "Demonstrating the importance of specificity in prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vague prompt\n",
    "vague_response = client.complete(\n",
    "    messages=[\n",
    "        UserMessage(content=\"Write a function to validate email.\")\n",
    "    ],\n",
    "    model=model,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(\"âŒ VAGUE PROMPT RESULT:\")\n",
    "print(\"=\"*60)\n",
    "print(vague_response.choices[0].message.content)\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specific prompt with context\n",
    "specific_response = client.complete(\n",
    "    messages=[\n",
    "        UserMessage(content=\"\"\"Write a Python function to validate email addresses with the following requirements:\n",
    "\n",
    "Function name: validate_email\n",
    "Parameters: email (str)\n",
    "Returns: tuple (bool, str) - (is_valid, error_message)\n",
    "\n",
    "Requirements:\n",
    "- Check for @ symbol presence\n",
    "- Validate domain has at least one dot\n",
    "- No spaces allowed\n",
    "- Local part (before @) must be 1-64 characters\n",
    "- Domain must be 1-255 characters\n",
    "- Return specific error messages for each validation failure\n",
    "\n",
    "Example usage:\n",
    "valid, msg = validate_email(\"user@example.com\")  # (True, \"\")\n",
    "valid, msg = validate_email(\"invalid\")  # (False, \"Missing @ symbol\")\n",
    "\"\"\")\n",
    "    ],\n",
    "    model=model,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(\"âœ… SPECIFIC PROMPT RESULT:\")\n",
    "print(\"=\"*60)\n",
    "print(specific_response.choices[0].message.content)\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Using System Messages for Context\n",
    "\n",
    "System messages set the behavior and context for the AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With system context\n",
    "response = client.complete(\n",
    "    messages=[\n",
    "        SystemMessage(content=\"\"\"You are an expert Python developer who writes secure, \n",
    "production-ready code following PEP 8 guidelines. Always include:\n",
    "- Type hints\n",
    "- Docstrings\n",
    "- Error handling\n",
    "- Input validation\n",
    "\"\"\"),\n",
    "        UserMessage(content=\"Write a function to parse a JSON file and extract user data.\")\n",
    "    ],\n",
    "    model=model,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(\"ðŸŽ¯ RESULT WITH SYSTEM CONTEXT:\")\n",
    "print(\"=\"*60)\n",
    "print(response.choices[0].message.content)\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Few-Shot Learning with Examples\n",
    "\n",
    "Providing examples to establish patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.inference.models import AssistantMessage\n",
    "\n",
    "# Few-shot prompting\n",
    "response = client.complete(\n",
    "    messages=[\n",
    "        SystemMessage(content=\"You convert natural language to SQL queries.\"),\n",
    "        UserMessage(content=\"Show all users\"),\n",
    "        AssistantMessage(content=\"SELECT * FROM users;\"),\n",
    "        UserMessage(content=\"Find users named John\"),\n",
    "        AssistantMessage(content=\"SELECT * FROM users WHERE name = 'John';\"),\n",
    "        UserMessage(content=\"Count active users from California\")\n",
    "    ],\n",
    "    model=model,\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "print(\"ðŸ“š FEW-SHOT LEARNING RESULT:\")\n",
    "print(\"=\"*60)\n",
    "print(response.choices[0].message.content)\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Role-Based Prompting\n",
    "\n",
    "Using specific roles to guide the AI's perspective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_to_review = \"\"\"\n",
    "def login(username, password):\n",
    "    user = db.execute(f\"SELECT * FROM users WHERE username='{username}'\")\n",
    "    if user and user['password'] == password:\n",
    "        session['user_id'] = user['id']\n",
    "        return True\n",
    "    return False\n",
    "\"\"\"\n",
    "\n",
    "# Security expert role\n",
    "security_review = client.complete(\n",
    "    messages=[\n",
    "        SystemMessage(content=\"\"\"You are a security expert reviewing code for vulnerabilities.\n",
    "Focus on: SQL injection, authentication flaws, password handling, and session management.\n",
    "Provide specific CVE references when applicable.\"\"\"),\n",
    "        UserMessage(content=f\"Review this login function for security issues:\\n\\n{code_to_review}\")\n",
    "    ],\n",
    "    model=model,\n",
    "    temperature=0.5\n",
    ")\n",
    "\n",
    "print(\"ðŸ”’ SECURITY EXPERT REVIEW:\")\n",
    "print(\"=\"*60)\n",
    "print(security_review.choices[0].message.content)\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Contextual Chaining\n",
    "\n",
    "Building context progressively across multiple prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = []\n",
    "\n",
    "# Step 1: Create base structure\n",
    "conversation.append(SystemMessage(content=\"You are a Python expert building a REST API.\"))\n",
    "conversation.append(UserMessage(content=\"Create a User model with id, email, and name fields using dataclasses.\"))\n",
    "\n",
    "response1 = client.complete(messages=conversation, model=model, temperature=0.7)\n",
    "conversation.append(AssistantMessage(content=response1.choices[0].message.content))\n",
    "\n",
    "print(\"ðŸ”¨ STEP 1: Base Model\")\n",
    "print(\"=\"*60)\n",
    "print(response1.choices[0].message.content)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Step 2: Add validation\n",
    "conversation.append(UserMessage(content=\"Add validation methods to ensure email is valid and name is not empty.\"))\n",
    "response2 = client.complete(messages=conversation, model=model, temperature=0.7)\n",
    "conversation.append(AssistantMessage(content=response2.choices[0].message.content))\n",
    "\n",
    "print(\"âœ… STEP 2: With Validation\")\n",
    "print(\"=\"*60)\n",
    "print(response2.choices[0].message.content)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Step 3: Add repository\n",
    "conversation.append(UserMessage(content=\"Create a UserRepository class with CRUD operations using this User model.\"))\n",
    "response3 = client.complete(messages=conversation, model=model, temperature=0.7)\n",
    "\n",
    "print(\"ðŸ—„ï¸ STEP 3: Repository Pattern\")\n",
    "print(\"=\"*60)\n",
    "print(response3.choices[0].message.content)\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Constraining Outputs\n",
    "\n",
    "Using constraints to control the format and content of responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.complete(\n",
    "    messages=[\n",
    "        SystemMessage(content=\"\"\"You generate Python code with these STRICT constraints:\n",
    "- Use type hints for all functions\n",
    "- Maximum function length: 20 lines\n",
    "- Include docstrings in Google style\n",
    "- Use descriptive variable names (no single letters except loop indices)\n",
    "- Prefer composition over inheritance\n",
    "- Always handle exceptions explicitly\"\"\"),\n",
    "        UserMessage(content=\"Create a function to calculate the Fibonacci sequence up to n terms.\")\n",
    "    ],\n",
    "    model=model,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(\"âš–ï¸ CONSTRAINED OUTPUT:\")\n",
    "print(\"=\"*60)\n",
    "print(response.choices[0].message.content)\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Token Usage Tracking\n",
    "\n",
    "Monitor token consumption to optimize costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare token usage between vague and specific prompts\n",
    "def get_token_usage(messages, model_name):\n",
    "    response = client.complete(messages=messages, model=model_name, temperature=0.7)\n",
    "    usage = response.usage\n",
    "    return {\n",
    "        'prompt_tokens': usage.prompt_tokens,\n",
    "        'completion_tokens': usage.completion_tokens,\n",
    "        'total_tokens': usage.total_tokens,\n",
    "        'response': response.choices[0].message.content\n",
    "    }\n",
    "\n",
    "# Vague prompt\n",
    "vague_result = get_token_usage(\n",
    "    [UserMessage(content=\"Write a sorting function\")],\n",
    "    model\n",
    ")\n",
    "\n",
    "# Specific prompt\n",
    "specific_result = get_token_usage(\n",
    "    [UserMessage(content=\"\"\"Write a Python function to sort a list of dictionaries by a specified key.\n",
    "    \n",
    "Function: sort_dict_list(items: List[dict], key: str, reverse: bool = False) -> List[dict]\n",
    "- Handle missing keys gracefully (move to end)\n",
    "- Support nested keys using dot notation (e.g., 'user.age')\n",
    "- Include type hints and docstring\n",
    "\"\"\")],\n",
    "    model\n",
    ")\n",
    "\n",
    "print(\"ðŸ“Š TOKEN USAGE COMPARISON:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Vague Prompt:\")\n",
    "print(f\"  Input:  {vague_result['prompt_tokens']} tokens\")\n",
    "print(f\"  Output: {vague_result['completion_tokens']} tokens\")\n",
    "print(f\"  Total:  {vague_result['total_tokens']} tokens\")\n",
    "print()\n",
    "print(f\"Specific Prompt:\")\n",
    "print(f\"  Input:  {specific_result['prompt_tokens']} tokens\")\n",
    "print(f\"  Output: {specific_result['completion_tokens']} tokens\")\n",
    "print(f\"  Total:  {specific_result['total_tokens']} tokens\")\n",
    "print()\n",
    "print(f\"Difference: {specific_result['total_tokens'] - vague_result['total_tokens']} tokens\")\n",
    "print(f\"Quality improvement: Specific prompt provides detailed requirements\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Key takeaways from prompt engineering:\n",
    "\n",
    "1. **Be Specific**: Detailed prompts yield better, more predictable results\n",
    "2. **Use System Messages**: Set context and constraints upfront\n",
    "3. **Provide Examples**: Few-shot learning establishes patterns\n",
    "4. **Use Roles**: Guide the AI's perspective and expertise\n",
    "5. **Chain Context**: Build complexity progressively\n",
    "6. **Set Constraints**: Control output format and style\n",
    "7. **Monitor Tokens**: Track usage for cost optimization\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Explore `04-context-optimization.ipynb` for token optimization strategies\n",
    "- Learn about RAG patterns in `05-rag-and-retrieval.ipynb`\n",
    "- Build complete pipelines in `06-context-pipeline.ipynb`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
