{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context Optimization and Token Management\n",
    "\n",
    "This notebook demonstrates advanced context optimization techniques including:\n",
    "- Chat reducers for conversation management\n",
    "- Progressive summarization\n",
    "- Token-efficient context building\n",
    "- Context window management\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import List, Dict, Tuple\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import SystemMessage, UserMessage, AssistantMessage\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "# Initialize client\n",
    "github_token = os.environ.get(\"GITHUB_TOKEN\")\n",
    "if not github_token:\n",
    "    raise ValueError(\"GITHUB_TOKEN environment variable must be set\")\n",
    "\n",
    "endpoint = \"https://models.github.ai/inference\"\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(github_token)\n",
    ")\n",
    "model = \"gpt-4o-mini\"\n",
    "\n",
    "print(f\"âœ… Setup complete - Using {model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Token Estimation\n",
    "\n",
    "Estimate tokens before making API calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_tokens(text: str) -> int:\n",
    "    \"\"\"Rough estimation: ~4 characters per token for English text.\"\"\"\n",
    "    return len(text) // 4\n",
    "\n",
    "def count_message_tokens(messages: List) -> int:\n",
    "    \"\"\"Estimate total tokens in a message list.\"\"\"\n",
    "    total = 0\n",
    "    for msg in messages:\n",
    "        if hasattr(msg, 'content'):\n",
    "            total += estimate_tokens(msg.content)\n",
    "        # Add overhead for message structure\n",
    "        total += 4\n",
    "    return total\n",
    "\n",
    "# Test estimation\n",
    "test_messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    UserMessage(content=\"Explain quantum computing in simple terms.\"),\n",
    "]\n",
    "\n",
    "estimated = count_message_tokens(test_messages)\n",
    "print(f\"ðŸ“Š Estimated tokens: {estimated}\")\n",
    "\n",
    "# Get actual count\n",
    "response = client.complete(messages=test_messages, model=model)\n",
    "actual = response.usage.prompt_tokens\n",
    "print(f\"ðŸŽ¯ Actual tokens: {actual}\")\n",
    "print(f\"ðŸ“ˆ Accuracy: {(1 - abs(estimated - actual) / actual) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chat Reducer - Sliding Window\n",
    "\n",
    "Keep only the most recent N messages to stay within token limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Message:\n",
    "    role: str\n",
    "    content: str\n",
    "    tokens: int\n",
    "    timestamp: datetime = field(default_factory=datetime.now)\n",
    "    important: bool = False\n",
    "\n",
    "class SlidingWindowReducer:\n",
    "    \"\"\"Keep only recent messages within token budget.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_tokens: int = 4000, window_size: int = 10):\n",
    "        self.max_tokens = max_tokens\n",
    "        self.window_size = window_size\n",
    "    \n",
    "    def reduce(self, messages: List[Message]) -> List[Message]:\n",
    "        # Always keep system message\n",
    "        system_msgs = [m for m in messages if m.role == 'system']\n",
    "        other_msgs = [m for m in messages if m.role != 'system']\n",
    "        \n",
    "        # Keep important messages\n",
    "        important_msgs = [m for m in other_msgs if m.important]\n",
    "        \n",
    "        # Take most recent messages\n",
    "        recent_msgs = other_msgs[-self.window_size:]\n",
    "        \n",
    "        # Combine and deduplicate\n",
    "        result = system_msgs + important_msgs\n",
    "        for msg in recent_msgs:\n",
    "            if msg not in result:\n",
    "                result.append(msg)\n",
    "        \n",
    "        # Check token budget\n",
    "        total_tokens = sum(m.tokens for m in result)\n",
    "        while total_tokens > self.max_tokens and len(result) > len(system_msgs) + 1:\n",
    "            # Remove oldest non-system, non-important message\n",
    "            for i, msg in enumerate(result):\n",
    "                if msg.role != 'system' and not msg.important:\n",
    "                    removed = result.pop(i)\n",
    "                    total_tokens -= removed.tokens\n",
    "                    break\n",
    "        \n",
    "        return result\n",
    "\n",
    "# Demo\n",
    "messages = [\n",
    "    Message('system', 'You are a helpful assistant.', 50),\n",
    "    Message('user', 'What is Python?', 20),\n",
    "    Message('assistant', 'Python is a programming language...', 100),\n",
    "    Message('user', 'What are decorators?', 25, important=True),\n",
    "    Message('assistant', 'Decorators are a design pattern...', 150),\n",
    "    Message('user', 'Show me an example', 30),\n",
    "    Message('assistant', 'Here is an example: @decorator...', 200),\n",
    "    Message('user', 'What about generators?', 25),\n",
    "    Message('assistant', 'Generators are functions that yield...', 180),\n",
    "]\n",
    "\n",
    "reducer = SlidingWindowReducer(max_tokens=400, window_size=4)\n",
    "reduced = reducer.reduce(messages)\n",
    "\n",
    "print(f\"ðŸ“‰ Original: {len(messages)} messages, {sum(m.tokens for m in messages)} tokens\")\n",
    "print(f\"ðŸ“Š Reduced: {len(reduced)} messages, {sum(m.tokens for m in reduced)} tokens\")\n",
    "print(f\"\\nâœ… Kept messages:\")\n",
    "for msg in reduced:\n",
    "    important_marker = \"â­\" if msg.important else \"  \"\n",
    "    print(f\"{important_marker} {msg.role:10s}: {msg.content[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Progressive Summarization\n",
    "\n",
    "Compress old conversation history into concise summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProgressiveSummarizer:\n",
    "    \"\"\"Compress conversation history into summaries.\"\"\"\n",
    "    \n",
    "    def __init__(self, client, model):\n",
    "        self.client = client\n",
    "        self.model = model\n",
    "    \n",
    "    def summarize_conversation(self, messages: List[Message], max_tokens: int = 200) -> str:\n",
    "        \"\"\"Create a summary of conversation history.\"\"\"\n",
    "        # Build conversation text\n",
    "        conversation_text = \"\\n\".join([\n",
    "            f\"{msg.role}: {msg.content}\" \n",
    "            for msg in messages \n",
    "            if msg.role in ['user', 'assistant']\n",
    "        ])\n",
    "        \n",
    "        # Request summary\n",
    "        response = self.client.complete(\n",
    "            messages=[\n",
    "                SystemMessage(content=f\"\"\"Summarize this conversation in {max_tokens} tokens or less.\n",
    "Focus on: key topics discussed, decisions made, important information shared.\n",
    "Format: Brief bullet points.\"\"\"),\n",
    "                UserMessage(content=conversation_text)\n",
    "            ],\n",
    "            model=self.model,\n",
    "            max_tokens=max_tokens\n",
    "        )\n",
    "        \n",
    "        return response.choices[0].message.content\n",
    "    \n",
    "    def hierarchical_compression(self, text: str, levels: List[int] = [500, 200, 50]) -> List[str]:\n",
    "        \"\"\"Create multiple compression levels.\"\"\"\n",
    "        summaries = []\n",
    "        current = text\n",
    "        \n",
    "        for max_tokens in levels:\n",
    "            response = self.client.complete(\n",
    "                messages=[\n",
    "                    SystemMessage(content=f\"Summarize in {max_tokens} tokens or less. Be concise.\"),\n",
    "                    UserMessage(content=current)\n",
    "                ],\n",
    "                model=self.model,\n",
    "                max_tokens=max_tokens\n",
    "            )\n",
    "            summary = response.choices[0].message.content\n",
    "            summaries.append(summary)\n",
    "            current = summary\n",
    "        \n",
    "        return summaries\n",
    "\n",
    "# Demo\n",
    "summarizer = ProgressiveSummarizer(client, model)\n",
    "\n",
    "# Simulate long conversation\n",
    "long_conversation = [\n",
    "    Message('user', 'I need to build an authentication system', 50),\n",
    "    Message('assistant', 'I can help with that. What features do you need?', 60),\n",
    "    Message('user', 'JWT tokens, refresh tokens, password hashing', 50),\n",
    "    Message('assistant', 'Great. I recommend using bcrypt for passwords...', 200),\n",
    "    Message('user', 'How do I implement refresh tokens?', 40),\n",
    "    Message('assistant', 'Store refresh tokens in a secure database...', 300),\n",
    "]\n",
    "\n",
    "summary = summarizer.summarize_conversation(long_conversation, max_tokens=100)\n",
    "\n",
    "print(\"ðŸ“ CONVERSATION SUMMARY:\")\n",
    "print(\"=\"*60)\n",
    "print(summary)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"Original: ~{sum(m.tokens for m in long_conversation)} tokens\")\n",
    "print(f\"Summary: ~{estimate_tokens(summary)} tokens\")\n",
    "print(f\"Compression: {(1 - estimate_tokens(summary) / sum(m.tokens for m in long_conversation)) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Context Window Manager\n",
    "\n",
    "Strategically allocate token budget across different context types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ContextBudget:\n",
    "    system_prompt: int = 500\n",
    "    conversation_history: int = 1000\n",
    "    retrieved_context: int = 1500\n",
    "    tool_outputs: int = 500\n",
    "    response_reserve: int = 1500\n",
    "    \n",
    "    @property\n",
    "    def total(self) -> int:\n",
    "        return (self.system_prompt + self.conversation_history + \n",
    "                self.retrieved_context + self.tool_outputs + self.response_reserve)\n",
    "\n",
    "class ContextWindowManager:\n",
    "    \"\"\"Manage token allocation across context types.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_tokens: int = 8000):\n",
    "        self.max_tokens = max_tokens\n",
    "    \n",
    "    def allocate(self, priorities: Dict[str, float]) -> ContextBudget:\n",
    "        \"\"\"Allocate tokens based on priority weights.\"\"\"\n",
    "        # Fixed allocations\n",
    "        system_tokens = 500\n",
    "        response_tokens = 1500\n",
    "        \n",
    "        # Remaining budget\n",
    "        remaining = self.max_tokens - system_tokens - response_tokens\n",
    "        \n",
    "        # Normalize priorities\n",
    "        total_priority = sum(priorities.values())\n",
    "        normalized = {k: v / total_priority for k, v in priorities.items()}\n",
    "        \n",
    "        return ContextBudget(\n",
    "            system_prompt=system_tokens,\n",
    "            conversation_history=int(remaining * normalized.get('history', 0.3)),\n",
    "            retrieved_context=int(remaining * normalized.get('retrieved', 0.5)),\n",
    "            tool_outputs=int(remaining * normalized.get('tools', 0.2)),\n",
    "            response_reserve=response_tokens\n",
    "        )\n",
    "    \n",
    "    def fit_to_budget(self, content: List[str], budget: int) -> List[str]:\n",
    "        \"\"\"Select content that fits within token budget.\"\"\"\n",
    "        result = []\n",
    "        tokens_used = 0\n",
    "        \n",
    "        for item in content:\n",
    "            item_tokens = estimate_tokens(item)\n",
    "            if tokens_used + item_tokens <= budget:\n",
    "                result.append(item)\n",
    "                tokens_used += item_tokens\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        return result\n",
    "\n",
    "# Demo\n",
    "manager = ContextWindowManager(max_tokens=8000)\n",
    "\n",
    "# Scenario 1: Code generation (need more examples)\n",
    "code_gen_budget = manager.allocate({\n",
    "    'history': 0.2,\n",
    "    'retrieved': 0.6,  # More examples and docs\n",
    "    'tools': 0.2\n",
    "})\n",
    "\n",
    "print(\"ðŸ’» CODE GENERATION BUDGET:\")\n",
    "print(f\"  System: {code_gen_budget.system_prompt} tokens\")\n",
    "print(f\"  History: {code_gen_budget.conversation_history} tokens\")\n",
    "print(f\"  Retrieved: {code_gen_budget.retrieved_context} tokens\")\n",
    "print(f\"  Tools: {code_gen_budget.tool_outputs} tokens\")\n",
    "print(f\"  Response: {code_gen_budget.response_reserve} tokens\")\n",
    "print(f\"  Total: {code_gen_budget.total} tokens\\n\")\n",
    "\n",
    "# Scenario 2: Conversation (need more history)\n",
    "chat_budget = manager.allocate({\n",
    "    'history': 0.7,  # More conversation context\n",
    "    'retrieved': 0.2,\n",
    "    'tools': 0.1\n",
    "})\n",
    "\n",
    "print(\"ðŸ’¬ CONVERSATION BUDGET:\")\n",
    "print(f\"  System: {chat_budget.system_prompt} tokens\")\n",
    "print(f\"  History: {chat_budget.conversation_history} tokens\")\n",
    "print(f\"  Retrieved: {chat_budget.retrieved_context} tokens\")\n",
    "print(f\"  Tools: {chat_budget.tool_outputs} tokens\")\n",
    "print(f\"  Response: {chat_budget.response_reserve} tokens\")\n",
    "print(f\"  Total: {chat_budget.total} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Semantic Caching\n",
    "\n",
    "Cache responses for semantically similar queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "\n",
    "class SemanticCache:\n",
    "    \"\"\"Simple semantic caching based on string similarity.\"\"\"\n",
    "    \n",
    "    def __init__(self, similarity_threshold: float = 0.85):\n",
    "        self.cache: Dict[str, Tuple[str, datetime]] = {}\n",
    "        self.threshold = similarity_threshold\n",
    "        self.ttl_seconds = 3600  # 1 hour\n",
    "    \n",
    "    def _similarity(self, a: str, b: str) -> float:\n",
    "        \"\"\"Calculate similarity between two strings.\"\"\"\n",
    "        return SequenceMatcher(None, a.lower(), b.lower()).ratio()\n",
    "    \n",
    "    def get(self, query: str) -> str | None:\n",
    "        \"\"\"Get cached response if similar query exists.\"\"\"\n",
    "        now = datetime.now()\n",
    "        \n",
    "        for cached_query, (response, timestamp) in self.cache.items():\n",
    "            # Check TTL\n",
    "            age = (now - timestamp).total_seconds()\n",
    "            if age > self.ttl_seconds:\n",
    "                continue\n",
    "            \n",
    "            # Check similarity\n",
    "            similarity = self._similarity(query, cached_query)\n",
    "            if similarity >= self.threshold:\n",
    "                return response\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def set(self, query: str, response: str):\n",
    "        \"\"\"Cache a query-response pair.\"\"\"\n",
    "        self.cache[query] = (response, datetime.now())\n",
    "    \n",
    "    def clear_expired(self):\n",
    "        \"\"\"Remove expired cache entries.\"\"\"\n",
    "        now = datetime.now()\n",
    "        expired = [\n",
    "            query for query, (_, timestamp) in self.cache.items()\n",
    "            if (now - timestamp).total_seconds() > self.ttl_seconds\n",
    "        ]\n",
    "        for query in expired:\n",
    "            del self.cache[query]\n",
    "\n",
    "# Demo\n",
    "cache = SemanticCache(similarity_threshold=0.85)\n",
    "\n",
    "# First query (cache miss)\n",
    "query1 = \"What is machine learning?\"\n",
    "cached = cache.get(query1)\n",
    "\n",
    "if not cached:\n",
    "    print(\"âŒ Cache miss - calling API\")\n",
    "    response1 = client.complete(\n",
    "        messages=[UserMessage(content=query1)],\n",
    "        model=model\n",
    "    )\n",
    "    result1 = response1.choices[0].message.content\n",
    "    cache.set(query1, result1)\n",
    "    tokens1 = response1.usage.total_tokens\n",
    "    print(f\"  Used {tokens1} tokens\\n\")\n",
    "\n",
    "# Similar query (cache hit)\n",
    "query2 = \"What is machine learning exactly?\"\n",
    "cached = cache.get(query2)\n",
    "\n",
    "if cached:\n",
    "    print(\"âœ… Cache hit - using cached response\")\n",
    "    print(f\"  Similarity: {cache._similarity(query1, query2):.2%}\")\n",
    "    print(f\"  Saved API call!\\n\")\n",
    "    result2 = cached\n",
    "    tokens2 = 0\n",
    "else:\n",
    "    print(\"âŒ Cache miss\")\n",
    "\n",
    "# Different query (cache miss)\n",
    "query3 = \"Explain quantum computing\"\n",
    "cached = cache.get(query3)\n",
    "\n",
    "if not cached:\n",
    "    print(\"âŒ Cache miss - calling API\")\n",
    "    print(f\"  Similarity to query1: {cache._similarity(query1, query3):.2%}\")\n",
    "    print(f\"  Below threshold ({cache.threshold:.2%})\\n\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Cache Stats:\")\n",
    "print(f\"  Entries: {len(cache.cache)}\")\n",
    "print(f\"  Hit rate: 1/2 = 50%\")\n",
    "print(f\"  Tokens saved: {tokens1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Complete Context Optimization Pipeline\n",
    "\n",
    "Combining all techniques for production use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedContextManager:\n",
    "    \"\"\"Complete context optimization system.\"\"\"\n",
    "    \n",
    "    def __init__(self, client, model, max_tokens=8000):\n",
    "        self.client = client\n",
    "        self.model = model\n",
    "        self.max_tokens = max_tokens\n",
    "        \n",
    "        self.reducer = SlidingWindowReducer(max_tokens=max_tokens)\n",
    "        self.summarizer = ProgressiveSummarizer(client, model)\n",
    "        self.window_manager = ContextWindowManager(max_tokens=max_tokens)\n",
    "        self.cache = SemanticCache()\n",
    "    \n",
    "    def process_request(self, \n",
    "                       user_query: str,\n",
    "                       conversation_history: List[Message],\n",
    "                       retrieved_docs: List[str],\n",
    "                       system_prompt: str = \"You are a helpful assistant.\") -> Dict:\n",
    "        \"\"\"Process a request with full optimization.\"\"\"\n",
    "        \n",
    "        # 1. Check cache\n",
    "        cached_response = self.cache.get(user_query)\n",
    "        if cached_response:\n",
    "            return {\n",
    "                'response': cached_response,\n",
    "                'cached': True,\n",
    "                'tokens_used': 0\n",
    "            }\n",
    "        \n",
    "        # 2. Reduce conversation history\n",
    "        reduced_history = self.reducer.reduce(conversation_history)\n",
    "        \n",
    "        # 3. Allocate token budget\n",
    "        budget = self.window_manager.allocate({\n",
    "            'history': 0.3,\n",
    "            'retrieved': 0.5,\n",
    "            'tools': 0.2\n",
    "        })\n",
    "        \n",
    "        # 4. Fit retrieved docs to budget\n",
    "        fitted_docs = self.window_manager.fit_to_budget(\n",
    "            retrieved_docs, \n",
    "            budget.retrieved_context\n",
    "        )\n",
    "        \n",
    "        # 5. Build context\n",
    "        messages = [SystemMessage(content=system_prompt)]\n",
    "        \n",
    "        # Add conversation summary if history is long\n",
    "        if len(conversation_history) > 10:\n",
    "            summary = self.summarizer.summarize_conversation(\n",
    "                conversation_history[:-5], \n",
    "                max_tokens=200\n",
    "            )\n",
    "            messages.append(SystemMessage(content=f\"Previous conversation summary:\\n{summary}\"))\n",
    "        \n",
    "        # Add recent history\n",
    "        for msg in reduced_history[-5:]:\n",
    "            if msg.role == 'user':\n",
    "                messages.append(UserMessage(content=msg.content))\n",
    "            elif msg.role == 'assistant':\n",
    "                messages.append(AssistantMessage(content=msg.content))\n",
    "        \n",
    "        # Add retrieved context\n",
    "        if fitted_docs:\n",
    "            context = \"\\n\\n\".join(fitted_docs)\n",
    "            messages.append(SystemMessage(content=f\"Relevant context:\\n{context}\"))\n",
    "        \n",
    "        # Add current query\n",
    "        messages.append(UserMessage(content=user_query))\n",
    "        \n",
    "        # 6. Make API call\n",
    "        response = self.client.complete(messages=messages, model=self.model)\n",
    "        result = response.choices[0].message.content\n",
    "        \n",
    "        # 7. Cache response\n",
    "        self.cache.set(user_query, result)\n",
    "        \n",
    "        return {\n",
    "            'response': result,\n",
    "            'cached': False,\n",
    "            'tokens_used': response.usage.total_tokens,\n",
    "            'history_reduced_from': len(conversation_history),\n",
    "            'history_reduced_to': len(reduced_history),\n",
    "            'docs_fitted_from': len(retrieved_docs),\n",
    "            'docs_fitted_to': len(fitted_docs)\n",
    "        }\n",
    "\n",
    "# Demo\n",
    "manager = OptimizedContextManager(client, model, max_tokens=4000)\n",
    "\n",
    "# Simulate a request\n",
    "history = [\n",
    "    Message('system', 'You are a Python expert.', 50),\n",
    "    Message('user', 'What are list comprehensions?', 30),\n",
    "    Message('assistant', 'List comprehensions are...', 100),\n",
    "    Message('user', 'Show me an example', 25),\n",
    "    Message('assistant', 'Here is an example...', 150),\n",
    "]\n",
    "\n",
    "docs = [\n",
    "    \"Python list comprehensions provide a concise way to create lists.\",\n",
    "    \"Syntax: [expression for item in iterable if condition]\",\n",
    "    \"List comprehensions are faster than traditional loops.\",\n",
    "]\n",
    "\n",
    "result = manager.process_request(\n",
    "    user_query=\"How do I filter even numbers using list comprehension?\",\n",
    "    conversation_history=history,\n",
    "    retrieved_docs=docs\n",
    ")\n",
    "\n",
    "print(\"ðŸŽ¯ OPTIMIZED REQUEST RESULT:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Response: {result['response'][:200]}...\")\n",
    "print(f\"\\nOptimization Stats:\")\n",
    "print(f\"  Cached: {result['cached']}\")\n",
    "print(f\"  Tokens used: {result['tokens_used']}\")\n",
    "print(f\"  History: {result['history_reduced_from']} â†’ {result['history_reduced_to']} messages\")\n",
    "print(f\"  Docs: {result['docs_fitted_from']} â†’ {result['docs_fitted_to']} items\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Techniques Demonstrated:\n",
    "\n",
    "1. **Token Estimation** - Predict costs before API calls\n",
    "2. **Sliding Window Reducer** - Keep recent + important messages\n",
    "3. **Progressive Summarization** - Compress old context\n",
    "4. **Context Window Management** - Strategic token allocation\n",
    "5. **Semantic Caching** - Avoid redundant API calls\n",
    "6. **Complete Pipeline** - All techniques combined\n",
    "\n",
    "### Benefits:\n",
    "\n",
    "- ðŸ“‰ **Reduced costs** through token optimization\n",
    "- âš¡ **Faster responses** via caching\n",
    "- ðŸŽ¯ **Better quality** through strategic context\n",
    "- ðŸ”„ **Scalability** for long conversations\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Explore `05-rag-and-retrieval.ipynb` for retrieval-augmented generation patterns."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
